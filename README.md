# Web-Crawler
Extracting web information

It is a java project that aims to crawl (extract html files) from a specific site and store this information in a specific directory on your computer.
It checks the permissions offered by the site for crawlers (links that can be accessed by the crawler) through the "robots.txt" file.
This project involves processing links (extracting the protocol, host and path), connection to the site, extracting and processing the permissions offered by the site, 
extracting, processing and storing links extracted from html files, storing processed links and highlighting processing performance (number of links processed per minute).
